{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Board\n",
    "class TicTacToeBoard:\n",
    "    def __init__(self, boardHeightWidth):\n",
    "\n",
    "        #Hyper-parameters to be used with the neural network agent\n",
    "        #Defines the rewards to be used i nthe Q function\n",
    "        self.winReward = 10\n",
    "        self.loseReward = -1\n",
    "        self.drawReward = 1;\n",
    "        self.moveReward = 0;\n",
    "        self.illegalMoveReward = -1\n",
    "        \n",
    "        #The tic tac toe board will always be square the have a 1D lenght of this\n",
    "        self.boardHeightWidth = boardHeightWidth\n",
    "        \n",
    "        #Define a blank game state board. The board must always be square. The stateBoard however\n",
    "        #will have 2 boards contained it is as defined by an M x 2N array.In a regular 3 row/col\n",
    "        #tic tac toe board this would mean there will be a 3 x 6 array. The first 3 rows containig\n",
    "        #entries for the X player and the last 3 rows containing entries for the O player.\n",
    "        #The initial state of the board will be all zeros\n",
    "        self.stateBoard = np.zeros([boardHeightWidth * 2, boardHeightWidth], dtype=int)\n",
    "        \n",
    "    def CheckWinCondition(self):\n",
    "        #Given the current state of the game board, checks  to see if a winning move has been made    \n",
    "        #Initialize winner. -1: No winner, 0: X wins, 1: O wins\n",
    "\n",
    "        #Check if 3 in a row horizontally    \n",
    "        for row in range(boardHeightWidth):\n",
    "            #Reset counters\n",
    "            xCounter = 0;\n",
    "            oCounter = 0;\n",
    "            for col in range(boardHeightWidth):\n",
    "                if self.stateBoard[row][col] == 1:\n",
    "                    xCounter += 1\n",
    "                if self.stateBoard[row + self.boardHeightWidth][col] == 1:\n",
    "                    oCounter += 1\n",
    "            if xCounter == self.boardHeightWidth:\n",
    "                winner = 0\n",
    "                #display(\"Winner by Horizonal\")\n",
    "                return winner\n",
    "            elif oCounter == self.boardHeightWidth:\n",
    "                winner = 1\n",
    "                #display(\"Winner by Horizontal\")\n",
    "                return winner\n",
    "\n",
    "        #Check if 3 in a row vertically\n",
    "        for col in range(self.boardHeightWidth):\n",
    "            #Reset counters\n",
    "            xCounter = 0;\n",
    "            oCounter = 0;\n",
    "            for row in range(self.boardHeightWidth):\n",
    "                if self.stateBoard[row][col] == 1:\n",
    "                    xCounter += 1\n",
    "                if self.stateBoard[row + self.boardHeightWidth][col] == 1:\n",
    "                    oCounter += 1\n",
    "            if xCounter == self.boardHeightWidth:\n",
    "                winner = 0\n",
    "                return winner\n",
    "            elif oCounter == self.boardHeightWidth:\n",
    "                winner = 1\n",
    "                return winner\n",
    " \n",
    "\n",
    "        #Check left to right diagonal\n",
    "        for colrow in range(self.boardHeightWidth):\n",
    "            #Reset counter \n",
    "            xCounter = 0\n",
    "            oCounter = 0\n",
    "            \n",
    "            if self.stateBoard[colrow][colrow] == 1:\n",
    "                 xCounter += 1\n",
    "            if self.stateBoard[colrow + self.boardHeightWidth][colrow] == 1:\n",
    "                 oCounter += 1                         \n",
    "        if xCounter == self.boardHeightWidth:\n",
    "            winner = 1\n",
    "            return winner\n",
    "        if oCounter == self.boardHeightWidth:\n",
    "            winner = 0\n",
    "            return winner\n",
    "        \n",
    "        \n",
    "        #Reset counter \n",
    "        xCounter = 0\n",
    "        oCounter = 0\n",
    "        \n",
    "        #Check right to left diagonal\n",
    "        colList = np.arange(self.boardHeightWidth-1, -1, -1).tolist()\n",
    "        for row in range(self.boardHeightWidth):\n",
    "            col = colList[row]\n",
    "            if self.stateBoard[row][col] == 1:\n",
    "                xCounter += 1\n",
    "            if self.stateBoard[row + self.boardHeightWidth][col] == 1:\n",
    "                oCounter += 1\n",
    "        if xCounter == self.boardHeightWidth:\n",
    "            winner = 1\n",
    "            return winner\n",
    "        if oCounter == self.boardHeightWidth:\n",
    "            winner = 0\n",
    "            return winner\n",
    "        \n",
    "        #If we make it here, there is no winner\n",
    "        return -1\n",
    "    \n",
    "    def CheckDrawCondition(self):\n",
    "        #Check to see if the board is completely full\n",
    "        \n",
    "        #Grab the compressed board which maries the X and O's state board\n",
    "        compressedBoard = self.CompressedBoard()\n",
    "        \n",
    "        #If the number of ones contained in the compressed board is equal to the number of squares\n",
    "        #on the game board then its a draw\n",
    "        if np.count_nonzero(compressedBoard) == (self.boardHeightWidth * self.boardHeightWidth):      \n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def MakeMove(self, row, col, playerNum):\n",
    "        #Each agent playing the game will call this function to make their move\n",
    "        #during their turn by entering the row and col they want to mark on th game board\n",
    "        #playerNum 0 = X, 1 = O\n",
    "        \n",
    "        #Check if the player is trying to place a mark in a column that is already\n",
    "        #occupied by a mark\n",
    "        if self.CheckIllegalMove(row, col) == True:\n",
    "            display(\"Move by player: \" + str(playerNum) + \" was illegal\")\n",
    "            return self.illegalMoveReward\n",
    "        \n",
    "        #Place their mark\n",
    "        if playerNum == 0:\n",
    "            self.stateBoard[row][col] = 1\n",
    "        elif playerNum == 1:\n",
    "            row = row + self.boardHeightWidth\n",
    "            self.stateBoard[row][col] = 1\n",
    "            \n",
    "        #Check if the move resulted in a game ending condition    \n",
    "        #Return the proper award if so\n",
    "        winner = self.CheckWinCondition()\n",
    "        if winner == 0 and playerNum == 0:\n",
    "            return self.winReward\n",
    "        elif winner == 1 and playerNum == 1:\n",
    "            return self.winReward\n",
    "        elif self.CheckDrawCondition() == True:\n",
    "            return self.drawReward\n",
    "        else:\n",
    "            return self.moveReward\n",
    "    \n",
    "    def CheckIllegalMove(self, row, col):\n",
    "        #sChecks if the player is trying to place a mark in a column that is already\n",
    "        #occupied by a mark\n",
    "        #Returns a boolean value indicating if the move is illegal\n",
    "        \n",
    "        #Split stateBoard into player components\n",
    "        splitBoard = np.split(self.stateBoard,2, axis=0)\n",
    "        xBoard = splitBoard[0]\n",
    "        oBoard = splitBoard[1]\n",
    "        combinedBoard = xBoard|oBoard\n",
    "        \n",
    "        if combinedBoard[row][col] == 1:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def CompressedBoard(self):\n",
    "    #Returns a stateBoard which has been collapsed down to a game board to show which spaces have something\n",
    "    #in them and which are empty. Takes a M x 2N array, splits it in half along the N axis and bitwise ors\n",
    "    #the two into an MxN array\n",
    "        splitBoard = np.split(self.stateBoard,2, axis=0)\n",
    "        xBoard = splitBoard[0]\n",
    "        oBoard = splitBoard[1]\n",
    "        combinedBoard = xBoard|oBoard\n",
    "        return combinedBoard\n",
    "    \n",
    "    def ResetBoard(self):\n",
    "        #Resets the state board at the end of each game\n",
    "        self.stateBoard = np.zeros([boardHeightWidth * 2, boardHeightWidth], dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomPlayer:\n",
    "#Define the random player\n",
    "#This player will only make random moves on the board\n",
    "\n",
    "    def MakeMove(self, combinedStateBoard):  \n",
    "    #Decides which squar it should fill at random given what is not\n",
    "    #already occupied. Return the row, col to be played\n",
    "       \n",
    "        #Find empty indices\n",
    "        indices = np.argwhere(combinedBoard == 0)\n",
    "        move = random.choice(indices)\n",
    "        return move[0], move[1]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QNeural Network Q Learning Player\n",
    "\n",
    "class NNPlayer:\n",
    "    def __init__(self, learningRate , discount, explorationRate, boardHeightWidth, numGames):    \n",
    "        \n",
    "        #Q Learning Hyper Parameters\n",
    "        self.discount = discount\n",
    "        self.learningRate = learningRate\n",
    "        self.discount = discount\n",
    "        self.explorationRate = explorationRate\n",
    "        self.explorationDelta = 1.0 / numGames    \n",
    "        \n",
    "        #Keep track of board dimensions for internal calculations\n",
    "        self.boardHeightWidth = boardHeightWidth\n",
    "        \n",
    "        #Neural Network Parameters\n",
    "        #Input layer will be the size of the  state board(game board x 2) since we track X e\n",
    "        #ntries and O entries indifferent parts of the array\n",
    "        self.inputLayerSize = 2 * boardHeightWidth * boardHeightWidth\n",
    "        #Output layer will be the size of the gameboard and indicate which square to fill\n",
    "        self.outputLayerSize = boardHeightWidth * boardHeightWidth\n",
    "        \n",
    "        #Set up tensorflow netwrok\n",
    "        self.session = tf.Session()\n",
    "        self.AssembleNetwork()\n",
    "        self.session.run(self.initializer)\n",
    "        \n",
    "        \n",
    "    def AssembleNetwork(self):\n",
    "        #Create the neural network architecture\n",
    "        \n",
    "        #Define how many neurons should be in each hidden layer\n",
    "        #For this exercise i chose to start testing with 2 layers \n",
    "        #starting off bigger in size than the input layer and\n",
    "        #tapering down a little in the second layer\n",
    "        layer1Size = self.inputLayerSize + 8\n",
    "        layer2Size = self.inputLayerSize + 2\n",
    "        \n",
    "        #Variable to hold the input values\n",
    "        self.nnInput = tf.placeholder(dtype = tf.float32, shape = [None, self.inputLayerSize])\n",
    "        \n",
    "        #Set up the hidden layers\n",
    "        fc1 = tf.layers.dense(self.nnInput, layer1Size, activation = tf.sigmoid, kernel_initializer = tf.constant_initializer(np.zeros((self.inputLayerSize, layer1Size))))\n",
    "        fc2 = tf.layers.dense(fc1, layer2Size, activation = tf.sigmoid, kernel_initializer = tf.constant_initializer(np.zeros((layer2Size, self.outputLayerSize))))\n",
    "        \n",
    "        self.nnOutput = tf.layers.dense(fc2, self.outputLayerSize)                     \n",
    "        self.targetOutput = tf.placeholder(shape=[None, self.outputLayerSize], dtype=tf.float32)\n",
    "        \n",
    "        #Setup up the loss function to figure out how far we are off in accurary\n",
    "        loss = tf.losses.mean_squared_error(self.targetOutput, self.nnOutput)\n",
    "        \n",
    "        #Setup our descent gradient optimizer\n",
    "        self.optimizer = tf.train.GradientDescentOptimizer(learning_rate=self.learningRate).minimize(loss)\n",
    "                              \n",
    "        self.initializer = tf.global_variables_initializer()\n",
    "        \n",
    "    def getQ(self, stateBoard):\n",
    "    \n",
    "        #One hot encode out state board so it can be put into the network\n",
    "        oneHotEncodedBoard = self.OneHotEncode(stateBoard)\n",
    "        \n",
    "        #Run the state board through the network and return the 1D array of output values\n",
    "        return self.session.run(self.nnOutput, feed_dict={self.nnInput: oneHotEncodedBoard})[0]\n",
    "    \n",
    "    def OneHotEncode(self, stateBoard):\n",
    "        #The state board is already in one hot format, just need to reshape it into a 1D array\n",
    "        reshapedBoard = stateBoard.reshape(-1)\n",
    "        reshapedBoard = reshapedBoard[None, :]\n",
    "        return reshapedBoard\n",
    "    \n",
    "    def RandomMove(self, stateBoard):\n",
    "        #Returns a row, col of a random move to be made given the available squared\n",
    "        splitBoard = np.split(stateBoard,2, axis=0)\n",
    "        xBoard = splitBoard[0]\n",
    "        oBoard = splitBoard[1]\n",
    "\n",
    "        combinedBoard = xBoard|oBoard\n",
    "        indices = np.argwhere(combinedBoard == 0)\n",
    "        move = random.choice(indices)\n",
    "        return move[0], move[1]\n",
    "    \n",
    "    def GetMove(self, stateBoard):\n",
    "    #Chooses which move to make\n",
    "        \n",
    "        #Exploration allows the neural net to avoid getting stuck in a rut based on intial weightings\n",
    "        #If rand returns a number lower than the current exploration rate, perform a random move which \n",
    "        #will help the network explore the rewards of paths it has not seen yet\n",
    "        if random.random() > self.explorationRate:\n",
    "            #Calculate a move based on Q learning\n",
    "            \n",
    "            #Grab Q array for current stateBoard\n",
    "            qArray = self.getQ(stateBoard)\n",
    "            \n",
    "            #Determine the set of legal moves\n",
    "            splitBoard = np.split(stateBoard,2, axis=0)\n",
    "            xBoard = splitBoard[0]\n",
    "            oBoard = splitBoard[1]\n",
    "            combinedBoard = xBoard|oBoard\n",
    "            indices = np.argwhere(combinedBoard == 0)\n",
    "            \n",
    "            #Create a list of row, col pairs that are legal moves that can be made\n",
    "            flattenedIndices =  list()\n",
    "            for rcSet in indices:\n",
    "                flattenedIndices.append(rcSet[0] * self.boardHeightWidth + rcSet[1])\n",
    "            \n",
    "            #Pick the index fromt the Q table which has the highest\n",
    "            #value  for the given set of valid choices. \n",
    "            #flattenedIndex will be the 1D index of the move to be made \n",
    "            flattenedIndex = flattenedIndices[np.argmax(qArray[flattenedIndices])]\n",
    " \n",
    "            \n",
    "            #Unflatted this index into  a row, col set\n",
    "            if flattenedIndex < self.boardHeightWidth - 1:\n",
    "                col = flattenedIndex\n",
    "                row = 0\n",
    "            else:\n",
    "                row =  flattenedIndex//self.boardHeightWidth\n",
    "                col = (flattenedIndex % self.boardHeightWidth)\n",
    "            \n",
    "            return int(row), int(col)\n",
    "        \n",
    "        else:\n",
    "            #Return a random move\n",
    "            return self.RandomMove(stateBoard)\n",
    "        \n",
    "    def Train(self, oldStateBoard, newStateBoard, action, reward):\n",
    "    #Trains the neural network according to the Q learning methedology\n",
    "    #Takes in the state of the board before making a move as well as what move was\n",
    "    #made, the state of the board after that move and the reward that was given for the move\n",
    "        \n",
    "        #Grab Q values for each state\n",
    "        oldStateQValues = self.getQ(oldStateBoard)\n",
    "        newStateQValues = self.getQ(newStateBoard)\n",
    "        \n",
    "        #Calculate the Q value given the future state, reward and discount hyper parameter\n",
    "        oldStateQValues[action] = reward + self.discount * np.amax(newStateQValues)\n",
    "        \n",
    "        #Set up neural net trainig data \n",
    "        trainingInput = self.OneHotEncode(oldStateBoard)\n",
    "        targetOutput = [oldStateQValues]\n",
    "        trainingData = {self.nnInput: trainingInput, self.targetOutput: targetOutput}\n",
    "        \n",
    "        #Train the model given the current moves\n",
    "        self.session.run(self.optimizer, feed_dict = trainingData)\n",
    "\n",
    "    def Update(self, oldStateBoard, newStateBoard, row, col, reward):\n",
    "        #Translate 2d row col values into a 1D index for th corresponding output layer index\n",
    "        flattenedIndex = row * self.boardHeightWidth + col\n",
    "        \n",
    "        #Train the mode\n",
    "        self.Train(oldStateBoard, newStateBoard, flattenedIndex, reward)\n",
    "        \n",
    "        #Lower the exploration rate. Early on, the model should do more random\n",
    "        #exploration to stop stagnation but as the model begins to get its feet under it\n",
    "        #lower that and let the q learnin take over\n",
    "        if (self.explorationRate - self.explorationDelta) >= 0:\n",
    "                self.explorationRate -= self.explorationDelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Stats for the past 300 games'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"X's won: 100  O's won: 51  Draws:150\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"Percentage X's won: 33.33333333333333%  Percentage O's won: 17.0%  Percentage Draws:50.0%\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Stats for the past 300 games'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"X's won: 118  O's won: 54  Draws:128\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"Percentage X's won: 39.33333333333333%  Percentage O's won: 18.0%  Percentage Draws:42.66666666666667%\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Stats for the past 300 games'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"X's won: 128  O's won: 57  Draws:115\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"Percentage X's won: 42.66666666666667%  Percentage O's won: 19.0%  Percentage Draws:38.333333333333336%\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Stats for the overal run of games'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"X's won: 346  O's won: 162  Draws:393\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Main Program\n",
    "\n",
    "#Q Learning Hyper-parameters\n",
    "learningRate = .1\n",
    "discount = .9\n",
    "explorationRate = 1.0\n",
    "\n",
    "#Number of games to train on\n",
    "numGames = 1001\n",
    "\n",
    "#Report out stastics of games won/lost/draw after this many games\n",
    "reportingPeriod = 300\n",
    "\n",
    "#Board dimensions\n",
    "boardHeightWidth = 5\n",
    "\n",
    "#Setup game board\n",
    "GameBoard = TicTacToeBoard(boardHeightWidth)\n",
    "gameOver = False\n",
    "\n",
    "#Set up our players\n",
    "RandomAgent = RandomPlayer()\n",
    "NNAgent = NNPlayer(learningRate , discount, explorationRate, boardHeightWidth, numGames)\n",
    "\n",
    "#Counters for keeping track of game statistics\n",
    "overallWinCounter = np.array([0, 0])\n",
    "subsectionWinCounter = np.array([0, 0])\n",
    "overallDrawCounter = 0;\n",
    "subsectionDrawCounter = 0;\n",
    "\n",
    "for gameNum in range (numGames):\n",
    "    #Reset Game Board\n",
    "    GameBoard.ResetBoard()\n",
    "\n",
    "    while True:\n",
    "        \n",
    "        #X makes move\n",
    "        board = GameBoard.stateBoard\n",
    "        oldStateBoard =  GameBoard.stateBoard\n",
    "        combinedBoard = GameBoard.CompressedBoard()\n",
    "        row, col = RandomAgent.MakeMove(combinedBoard)\n",
    "        reward = GameBoard.MakeMove(row, col, 1)\n",
    "        newStateBoard = GameBoard.stateBoard\n",
    "\n",
    "        winner = GameBoard.CheckWinCondition()\n",
    "        draw = GameBoard.CheckDrawCondition()\n",
    "        if winner != -1:\n",
    "            #print(\"We have a winner: Player \" + str(winner))\n",
    "            #display(\"Game # \" + str(gameNum))\n",
    "            #display(board)\n",
    "            subsectionWinCounter[winner] += 1\n",
    "            #If the opponent wins, the model needs to train itself on the loss\n",
    "            #Gives the negative of the reward the opponent got\n",
    "            NNAgent.Update(oldStateBoard, newStateBoard, nnRow, nnCol, -reward)\n",
    "            break\n",
    "        elif draw == True:\n",
    "            #Game ended in draw\n",
    "            #print(\"Game ended in draw\")\n",
    "            subsectionDrawCounter += 1\n",
    "            break   \n",
    "        \n",
    "        #O makes move\n",
    "        oldStateBoard =  GameBoard.stateBoard\n",
    "        nnRow, nnCol = NNAgent.GetMove(GameBoard.stateBoard)\n",
    "        reward = GameBoard.MakeMove(nnRow, nnCol, 0)\n",
    "        newStateBoard = GameBoard.stateBoard\n",
    "        NNAgent.Update(oldStateBoard, newStateBoard, nnRow, nnCol, reward)\n",
    "        \n",
    "        winner = GameBoard.CheckWinCondition()\n",
    "        draw = GameBoard.CheckDrawCondition()\n",
    "        if winner != -1:\n",
    "            #print(\"We have a winner: Player \" + str(winner))\n",
    "            #display(\"Game # \" + str(gameNum))\n",
    "            #isplay(board)\n",
    "            subsectionWinCounter[winner] += 1\n",
    "            break\n",
    "        elif draw == True:\n",
    "            #print(\"Game ended in draw\")\n",
    "            subsectionDrawCounter += 1\n",
    "            break   \n",
    "        \n",
    "    #Report out game statistics per the reporting period\n",
    "    if gameNum % reportingPeriod == 0 and gameNum != 0:\n",
    "        overallWinCounter[0] += subsectionWinCounter[0]\n",
    "        overallWinCounter[1] += subsectionWinCounter[1]\n",
    "        overallDrawCounter += subsectionDrawCounter\n",
    "        display(\"Stats for the past \" + str(reportingPeriod) + \" games\")\n",
    "        display(\"X's won: \" + str(subsectionWinCounter[0]) + \"  O's won: \" + str(subsectionWinCounter[1]) + \"  Draws:\" + str(subsectionDrawCounter)) \n",
    "        display(\"Percentage X's won: \" + str((subsectionWinCounter[0]/reportingPeriod)*100) + \"%  Percentage O's won: \" + str((subsectionWinCounter[1]/reportingPeriod)*100) + \"%  Percentage Draws:\" + str((subsectionDrawCounter/reportingPeriod)*100) + \"%\") \n",
    "        \n",
    "        #Reset the reporting period specific counters\n",
    "        subsectionWinCounter[0] = 0\n",
    "        subsectionWinCounter[1] = 0\n",
    "        subsectionDrawCounter = 0\n",
    "        \n",
    "#Report out overall stats across all games\n",
    "display(\"Stats for the overal run of games\")\n",
    "display(\"X's won: \" + str(overallWinCounter[0]) + \"  O's won: \" + str(overallWinCounter[1]) + \"  Draws:\" + str(overallDrawCounter))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
